# DocuMind AI --- RAG Document Assistant (Local LLM + Streamlit)

An LLM-powered document search and question-answering assistant built
using **Retrieval-Augmented Generation (RAG)**.\
It ingests PDFs, chunks them, generates embeddings, stores them in a
vector database (Chroma), retrieves the most relevant chunks at query
time, and generates grounded answers with citations.

------------------------------------------------------------------------

## ğŸš€ Project Capabilities

-   PDF ingestion (multiple documents)
-   Recursive chunking with overlap
-   Semantic embeddings generation
-   Chroma vector database indexing
-   Top-K similarity retrieval
-   Context-grounded answer generation
-   Citation tracking (source file + page)
-   Streamlit interactive UI
-   Local LLM inference via Ollama (Llama 3.1)

------------------------------------------------------------------------

## ğŸ§  Architecture Overview

PDFs â†’ Loader â†’ Chunking â†’ Embeddings â†’ Chroma Vector DB\
User Query â†’ Embed â†’ Retrieve Top-K â†’ Prompt (Context + Question) â†’ LLM
â†’ Answer + Sources

------------------------------------------------------------------------

## ğŸ“ Repository Structure

rag-document-assistant/ â”‚ â”œâ”€â”€ app/ â”‚ â””â”€â”€ app.py \# Streamlit UI â”‚ â”œâ”€â”€
src/ â”‚ â”œâ”€â”€ config.py \# Config constants â”‚ â”œâ”€â”€ providers.py \# LLM +
Embedding provider logic â”‚ â”œâ”€â”€ ingest.py \# PDF ingestion pipeline â”‚ â”œâ”€â”€
retriever.py \# Vector search logic â”‚ â”œâ”€â”€ rag.py \# RAG chain logic â”‚
â”œâ”€â”€ agent_graph.py \# Placeholder for LangGraph workflows â”‚ â””â”€â”€ utils.py
\# Helper utilities â”‚ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ raw_docs/ \# Add PDFs here â”‚ â””â”€â”€
chroma_db/ \# Persisted vector database â”‚ â”œâ”€â”€ .env.example â”œâ”€â”€
requirements.txt â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ Local Setup

### 1ï¸âƒ£ Clone and create environment

``` bash
git clone https://github.com/Kollipara-Hema/rag-document-assistant.git
cd rag-document-assistant
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install Ollama (Local LLM)

Download from: https://ollama.com

Pull the model:

``` bash
ollama pull llama3.1
```

Test model:

``` bash
ollama run llama3.1
```

Exit with `Ctrl + D`

------------------------------------------------------------------------

## ğŸ“š Add Documents

Place your PDFs inside:

    data/raw_docs/

------------------------------------------------------------------------

## ğŸ”„ Build the Vector Index

``` bash
python -u -m src.ingest
```

This will: - Load PDFs - Chunk text - Generate embeddings - Store
vectors in Chroma

------------------------------------------------------------------------

## ğŸ’¬ Ask Questions (CLI)

``` bash
python -u -m src.rag
```

------------------------------------------------------------------------

## ğŸ–¥ Run Streamlit App

``` bash
streamlit run app/app.py
```

Open in browser: http://localhost:8501

------------------------------------------------------------------------

## ğŸ§© Engineering Highlights

-   Modular provider abstraction (LLM & embeddings)
-   Grounded response prompting to reduce hallucination
-   Metadata-based citation tracking
-   Designed for local-first experimentation
-   Easily extendable to OpenAI or hosted LLM providers

------------------------------------------------------------------------

## ğŸ“Œ Deployment Note

Streamlit Community Cloud does NOT support Ollama runtime.\
For public deployment, switch to OpenAI or another hosted provider.

------------------------------------------------------------------------

## ğŸ›  Future Improvements

-   Hybrid search (BM25 + vector)
-   Reranking layer
-   LangGraph multi-step reasoning
-   Evaluation metrics (faithfulness, retrieval recall)
-   Cloud-ready deployment pipeline


